# ⚡️ 计算机设计大赛参赛方案：盘中异动极速归因聚合引擎 (Anomaly Attribution Engine)

## 一、 项目背景与真实痛点（绝不喊口号）
在真实的量化交易与主观交易交织的A股市场中，交易员和分析师面临的最大痛点并非“缺乏复杂的预测模型”，而是**“极度严重的信息与时间断层（Information/Time Disconnect）”**。

当一只股票在盘中突发异动（如：10秒内拉升或跳水5%以上）时，真实的导火索往往散落并碎片化在全网的各个角落（如某篇自媒体爆料、一条刚刚发酵的股吧传闻、一份晦涩的上下游公司公告）。此时，对于人类研究员而言，**确认波动的原因需要数分钟的跨平台搜索和梳理，而在这关键的几分钟里，最好的交易/撤离窗口已经关闭。**

本项目《盘中异动极速归因聚合引擎》(Anomaly Attribution Engine) 致力于解决这一脏活与难题：**我们将彻底摒弃不切实际的“用情绪预测涨跌”的老路，致力于把所有非结构化的全网资讯流与高频/毫秒级的股票Tick流，在时序数据库（Time-Series Database）中进行亚秒级的“精准强制对齐（Alignment）”，并利用本地大模型进行“瞬时归因图谱构建”。**

这是一个切中行业极速痛点、拥有完整闭环、极具工程挑战性的纯正“计算机大数据”赛道方案。

---

## 二、 核心架构重构（剥离花哨，回归工程）

本项目展现的核心不在于花哨的3D，而在于**高并发吞吐、毫秒级时序对齐与大模型信息抽取的深度融合**。

### 1. 异构数据“强制对齐”底座 (Time-Series Alignment Engine)
*   以往大多数NLP项目都是静态的文本分析。本系统采用 **TDengine 或 InfluxDB** 这个级别的工业物联网时序数据库。
*   将爬虫矩阵（Scraping Matrix）抓取到的股吧发帖、快讯摘要，与毫秒级的交易 K线快照，强制打上统一的**绝对时间戳**。
*   通过大数据的 Join 操作，能够瞬间回放：在股票异常拉大阳线的前后 5 分钟内，社交网络和资讯源上究竟发生了什么微观变化，彻底缝合信息断层。

### 2. LLM知识图谱溯源层 (Information Provenance via LLM)
*   **非判断型 AI**：大模型在这里不负责输出“看跌/看涨”这种伪命题，而是负责做高并发的**命名实体识别（NER）和关系抽取（RE）**。
*   当系统捕获异动时，调动本地部署的轻量级大模型（如 Qwen2.5），从异动前2小时的海量降噪文本中提取核心关键词与因果逻辑，并利用 **NetworkX或Neo4j** 构建“瞬时异动知识图谱”。

### 3. “高信息密度”可视化大屏 (High-Density Professional UI)
*   摒弃为了比赛而做的华而不实的 3D 动画，采用真正类似 Bloomberg Terminal 的**硬核投研级暗色调UI**（以黑/灰/红/绿为主配色）。
*   左侧不仅有K线，还有高频逐笔交易的“资金流瀑布”。
*   右侧独创性加入**“多模态归因时序流（Attribution Feed）”**。鼠标停留在分时图的异动尖刺，旁边瞬间拉出由大模型秒级生成的“异动因果逻辑树”这才是让评委和业界都感到惊艳的“可用之物”。

---

## 三、 对比同类获奖项目的“降维实力”

1. **工程能力的碾压**：
    普通的比赛项目往往只停留在“把CSV塞给大模型调取API生成一段话”的水平。本方案展示了在处理“高并发Tick流”及“多源异构流对齐”时的极速处理能力，这是真正属于**大数据体系（Big Data Pipeline）**的硬实力，契合赛道要求。
2. **逻辑上的自洽与真实商业价值**：
    不承担“神棍预测”的责任，这就完全堵住了评委“你怎么可能比华尔街还准”的质疑。我们的闭环是完美的：“我是一个辅助人类决策的极速探照灯，帮人类省下了那几分钟最宝贵的信息搜集时间。”这个立意，既成熟又谦逊，极为容易拿高分。

---

## 四、 本阶段执行规划

系统架构图和文档的迭代将严格按照上述极客化、工程化的思路重构：
1. **清理与重构前端（Frontend 重写）**：拆除之前堆叠的无意义 3D 组件，转向高信息密度的模块（如 TradingView Lightweight Charts + 瞬时归因卡片流提取）。
2. **搭建时间戳对齐伪骨架（Backend 换血）**：在后端构建能模拟高频数据流输入、并强行进行 Time-Window Joins 的路由与服务逻辑，以及大模型知识图谱生成模块。
