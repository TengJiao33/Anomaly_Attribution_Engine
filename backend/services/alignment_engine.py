"""
æ—¶åºå¯¹é½å¼•æ“ (Alignment Engine Service)

æ ¸å¿ƒæœåŠ¡ï¼šé‡æ„ç‰ˆã€‚çœŸæ­£é›†æˆï¼š
1. SQLite æ—¶åºå­˜å‚¨ï¼ˆæ›¿ä»£é™æ€ JSONï¼‰
2. AnomalyDetector ç»Ÿè®¡å­¦å¼‚åŠ¨æ£€æµ‹ï¼ˆæ›¿ä»£ç¡¬ç¼–ç æ ‡è®°ï¼‰
3. AttributionAnalyzer å¤§æ¨¡å‹å½’å› ï¼ˆçœŸæ­£è°ƒç”¨ LLMï¼‰
4. RedisCache ç¼“å­˜å±‚ï¼ˆLLM ç»“æœç¼“å­˜ + ç³»ç»ŸæŒ‡æ ‡ï¼‰
"""

import asyncio
import json
import os
import time
from datetime import datetime, timedelta
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from loguru import logger

from services.timeseries_db import TimeSeriesDB
from services.multi_signal_detector import MultiSignalDetector
from services.redis_cache import RedisCache
from ai_engine.attribution_analyzer import AttributionAnalyzer


class AlignmentDataPoint(BaseModel):
    timestamp: str
    open: float
    high: float
    low: float
    close: float
    volume: float
    hasAnomaly: bool
    anomalyDetails: Optional[Dict] = None
    detectionStats: Optional[Dict] = None  # æ–°å¢ï¼šæ£€æµ‹ç»Ÿè®¡ä¿¡æ¯


class AlignmentResponse(BaseModel):
    symbol: str
    symbolName: str
    data: List[AlignmentDataPoint]


class CaseInfo(BaseModel):
    case_id: str
    symbol: str
    symbol_name: str
    case_date: str
    description: str
    anomaly_type: str
    tick_count: int = 0
    news_count: int = 0


class AlignmentEngineService:
    """æ—¶åºå¯¹é½å¼•æ“ â€” æ ¸å¿ƒæœåŠ¡"""

    def __init__(self):
        self.cases_dir = os.path.join(os.path.dirname(__file__), '..', 'data', 'cases')
        self.cases_index = self._load_cases_index()
        self.analyzer = AttributionAnalyzer()
        self.cache = RedisCache(
            host=os.environ.get("REDIS_HOST", "localhost"),
            port=int(os.environ.get("REDIS_PORT", 6379))
        )
        # ç³»ç»Ÿäº‹ä»¶æ—¥å¿—ï¼ˆä¾›å‰ç«¯ TickerTape æ¶ˆè´¹ï¼‰
        self.system_events: List[Dict] = []
        self._add_event("system", "æ—¶åºå¯¹é½å¼•æ“åˆå§‹åŒ–å®Œæˆ")
        self._add_event("system", f"å·²åŠ è½½ {len(self.cases_index)} ä¸ªå†å²æ¡ˆä¾‹")
        
        logger.info(f"AlignmentEngine åˆå§‹åŒ–å®Œæˆï¼Œ{len(self.cases_index)} ä¸ªæ¡ˆä¾‹å°±ç»ª")

    def _load_cases_index(self) -> List[Dict]:
        """åŠ è½½æ¡ˆä¾‹ç´¢å¼•"""
        index_path = os.path.join(self.cases_dir, "cases_index.json")
        try:
            with open(index_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.warning("æ¡ˆä¾‹ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ scripts/prepare_case.py")
            return []

    def _get_case_by_symbol(self, symbol: str) -> Optional[Dict]:
        """æ ¹æ®æ ‡çš„ä»£ç æŸ¥æ‰¾æ¡ˆä¾‹ï¼ˆå–ç¬¬ä¸€ä¸ªåŒ¹é…ï¼‰"""
        for case in self.cases_index:
            if case["symbol"] == symbol:
                return case
        return None

    def _get_case_by_id(self, case_id: str) -> Optional[Dict]:
        """æ ¹æ®æ¡ˆä¾‹ ID æŸ¥æ‰¾"""
        for case in self.cases_index:
            if case["case_id"] == case_id:
                return case
        return None

    def _open_case_db(self, case_id: str) -> TimeSeriesDB:
        """æ‰“å¼€æŒ‡å®šæ¡ˆä¾‹çš„ SQLite æ•°æ®åº“"""
        db_path = os.path.join(self.cases_dir, case_id, "timeseries.db")
        return TimeSeriesDB(db_path)

    def _load_precomputed_kg(self, case_id: str) -> Optional[Dict]:
        """åŠ è½½é¢„è®¡ç®—çš„çŸ¥è¯†å›¾è°±ï¼ˆLLM fallbackï¼‰"""
        kg_path = os.path.join(self.cases_dir, case_id, "precomputed_kg.json")
        try:
            with open(kg_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return None

    def _add_event(self, event_type: str, message: str):
        """æ·»åŠ ç³»ç»Ÿäº‹ä»¶"""
        event = {
            "time": datetime.now().strftime("%H:%M:%S"),
            "type": event_type,
            "message": message
        }
        self.system_events.append(event)
        # ä¿ç•™æœ€è¿‘ 50 æ¡
        if len(self.system_events) > 50:
            self.system_events = self.system_events[-50:]

    # ===== å…¬å¼€ API ===== #

    def get_available_cases(self) -> List[CaseInfo]:
        """è·å–æ‰€æœ‰å¯ç”¨æ¡ˆä¾‹åˆ—è¡¨"""
        return [CaseInfo(**case) for case in self.cases_index]

    def get_system_events(self) -> List[Dict]:
        """è·å–ç³»ç»Ÿäº‹ä»¶æ—¥å¿—"""
        return self.system_events[-20:]

    def get_system_metrics(self) -> Dict:
        """è·å–ç³»ç»Ÿè¿è¡ŒæŒ‡æ ‡"""
        return self.cache.get_system_metrics()

    async def generate_historical_alignment(
        self, case_id: str
    ) -> AlignmentResponse:
        """
        è·å–æŒ‡å®šæ¡ˆä¾‹çš„åˆå§‹ä¸Šä¸‹æ–‡æ•°æ®ï¼ˆå‰ç«¯é¦–æ¬¡åŠ è½½ï¼‰ã€‚
        å–åˆ‡ç‰‡æ•°æ®çš„å‰ N æ¡ä½œä¸ºåˆå§‹å¿«ç…§ã€‚
        """
        case = self._get_case_by_id(case_id)
        if not case:
            # å°è¯•æŒ‰ symbol æŸ¥æ‰¾ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
            case = self._get_case_by_symbol(case_id)
        
        if not case:
            logger.warning(f"æ¡ˆä¾‹ä¸å­˜åœ¨: {case_id}")
            return AlignmentResponse(symbol=case_id, symbolName="Unknown", data=[])

        db = self._open_case_db(case["case_id"])
        ticks = db.get_ticks(case["symbol"], limit=500)
        db.close()

        if not ticks:
            return AlignmentResponse(
                symbol=case["symbol"],
                symbolName=case["symbol_name"],
                data=[]
            )

        # åªè¿”å›å‰3æ¡ä½œä¸ºåˆå§‹ä¸Šä¸‹æ–‡ï¼ˆåç»­é€šè¿‡ WebSocket é€æ¡æ¨é€ï¼‰
        initial_ticks = ticks[:3]
        now = datetime.now()

        data_points = []
        for i, t in enumerate(initial_ticks):
            offset_seconds = (len(initial_ticks) - i) * 2
            ts_str = (now - timedelta(seconds=offset_seconds)).strftime("%H:%M:%S.000")
            
            data_points.append(AlignmentDataPoint(
                timestamp=ts_str,
                open=t["open"], high=t["high"],
                low=t["low"], close=t["close"],
                volume=t["volume"],
                hasAnomaly=False
            ))

        self._add_event("replay", f"åŠ è½½æ¡ˆä¾‹ [{case['symbol_name']}] åˆå§‹å¿«ç…§")

        return AlignmentResponse(
            symbol=case["symbol"],
            symbolName=case["symbol_name"],
            data=data_points
        )

    async def stream_alignment_feed(self, case_id: str, control_state: dict = None):
        """
        æ ¸å¿ƒï¼šå†å²åˆ‡ç‰‡å›æ”¾å¼•æ“ + å®æ—¶å¼‚åŠ¨æ£€æµ‹ + LLM å½’å› 
        
        æ•°æ®æµï¼š
        SQLite Ticks â†’ MultiSignalDetector â†’ [å¼‚åŠ¨è§¦å‘] â†’ æ—¶é—´çª—å£æŸ¥è¯¢èµ„è®¯ â†’ LLM å½’å› 
        
        :param control_state: å›æ”¾æ§åˆ¶çŠ¶æ€ {paused: bool, speed: float}ï¼Œç”± WebSocket ç«¯åŠ¨æ€æ›´æ–°
        """
        if control_state is None:
            control_state = {"paused": False, "speed": 1.0}
        case = self._get_case_by_id(case_id)
        if not case:
            case = self._get_case_by_symbol(case_id)
        if not case:
            logger.error(f"æµå¼å›æ”¾å¤±è´¥ï¼šæ¡ˆä¾‹ä¸å­˜åœ¨ {case_id}")
            return

        db = self._open_case_db(case["case_id"])
        ticks = db.get_ticks(case["symbol"], limit=500)

        if len(ticks) <= 3:
            db.close()
            return

        # è·³è¿‡åˆå§‹åŒ–æ¨é€çš„å‰3æ¡ï¼ˆä»…ç”¨äºæ£€æµ‹å™¨é¢„çƒ­ï¼Œä½†ä¹Ÿæ¨é€ç»™å‰ç«¯æ˜¾ç¤ºKçº¿ï¼‰
        warmup_ticks = ticks[:3]
        replay_ticks = ticks[3:]
        
        # åˆå§‹åŒ–å¼‚åŠ¨æ£€æµ‹å™¨ï¼ˆä½¿ç”¨å‰3æ¡åšé¢„çƒ­ï¼‰
        detector = MultiSignalDetector(
            window_size=5, z_threshold=1.5, volume_surge=2.0,
            cusum_drift=0.003, cusum_h=0.01,
            amihud_surge=2.0, posterior_threshold=0.35
        )
        for t in ticks[:3]:
            detector.feed(t["price"], t["volume"])

        # åŠ è½½é¢„è®¡ç®—çŸ¥è¯†å›¾è°±ä½œä¸º fallback
        precomputed_kg = self._load_precomputed_kg(case["case_id"])

        self._add_event("replay", f"å¼€å§‹å›æ”¾ [{case['symbol_name']}]ï¼Œå…± {len(ticks)} ä¸ª Tick")
        self.cache.increment_metric("ws_connections")

        queue = asyncio.Queue()

        async def tick_producer():
            # å…ˆæ¨é€é¢„çƒ­æ•°æ®ï¼ˆéå¼‚åŠ¨ï¼Œç”¨äºå‰ç«¯ç»˜åˆ¶å®Œæ•´Kçº¿ï¼‰
            for t in warmup_ticks:
                now = datetime.now()
                ts_str = now.strftime("%H:%M:%S.") + f"{now.microsecond // 1000:03d}"
                point_data = AlignmentDataPoint(
                    timestamp=ts_str,
                    open=round(t["open"], 2),
                    high=round(t["high"], 2),
                    low=round(t["low"], 2),
                    close=round(t["close"], 2),
                    volume=round(t["volume"], 0),
                    hasAnomaly=False,
                    anomalyDetails=None,
                    detectionStats=None
                )
                await queue.put(point_data.model_dump())
                self.cache.increment_metric("total_ticks_pushed")
                await asyncio.sleep(0.15)  # å¿«é€Ÿæ¨é€é¢„çƒ­æ•°æ®

            # å†æ¨é€æ£€æµ‹æ•°æ®
            for t in replay_ticks:
                now = datetime.now()
                ts_str = now.strftime("%H:%M:%S.") + f"{now.microsecond // 1000:03d}"

                # ---- å¼‚åŠ¨æ£€æµ‹ï¼ˆçœŸæ­£çš„ç®—æ³•ï¼‰ ---- #
                detection = detector.feed(t["price"], t["volume"])
                is_anomaly = detection["is_anomaly"]

                anomaly_details = None
                if is_anomaly:
                    self.cache.increment_metric("anomalies_detected")
                    prob = detection.get('anomaly_probability', 0)
                    self._add_event("anomaly",
                        f"æ£€æµ‹åˆ° {case['symbol']} å¼‚åŠ¨! "
                        f"P(anomaly)={prob:.1%}, Z={detection['z_score']:.2f}Ïƒ, "
                        f"é‡æ¯”={detection['volume_ratio']:.1f}x"
                    )
                    logger.info(
                        f"[{ts_str}] ğŸš¨ å¼‚åŠ¨è§¦å‘ | {case['symbol']} | "
                        f"P={prob:.1%} | Z={detection['z_score']:.2f}Ïƒ | "
                        f"Vol={detection['volume_ratio']:.1f}x"
                    )

                    # ---- æ—¶é—´çª—å£å¯¹é½ï¼šæŸ¥è¯¢å¼‚åŠ¨å‰åçš„èµ„è®¯ ---- #
                    aligned_news = db.get_aligned_news(
                        case["symbol"], t["timestamp"],
                        window_before_sec=120, window_after_sec=30
                    )
                    
                    if aligned_news:
                        news_text = "\n".join([
                            f"[{n['timestamp']}][{n['source']}] {n['content']}"
                            for n in aligned_news
                        ])
                        self._add_event("alignment",
                            f"æ—¶åºå¯¹é½å®Œæˆï¼š{t['timestamp']} å‰åçª—å£å‘½ä¸­ {len(aligned_news)} æ¡èµ„è®¯"
                        )
                    else:
                        news_text = ""

                    # ---- LLM å½’å› ï¼ˆå¸¦ç¼“å­˜ + fallbackï¼‰ ---- #
                    attribution_source = "precomputed"
                    
                    if news_text:
                        # å…ˆæŸ¥ç¼“å­˜
                        cached = self.cache.get_cached_kg(news_text)
                        if cached:
                            anomaly_details = cached
                            attribution_source = "cached"
                            self._add_event("llm", "å‘½ä¸­å½’å› ç¼“å­˜ [Cache Hit]")
                        else:
                            # å°è¯•å®æ—¶è°ƒç”¨ LLM
                            try:
                                llm_start = time.time()
                                kg_result = await self.analyzer.extract_knowledge_graph(news_text)
                                llm_latency = (time.time() - llm_start) * 1000
                                
                                self.cache.increment_metric("llm_calls")
                                self.cache.set_metric("avg_llm_latency_ms", round(llm_latency))
                                
                                if kg_result and "summary" in kg_result:
                                    anomaly_details = kg_result
                                    self.cache.set_cached_kg(news_text, kg_result)
                                    attribution_source = "live_llm"
                                    self._add_event("llm",
                                        f"LLM å®æ—¶å½’å› å®Œæˆï¼Œå»¶è¿Ÿ {llm_latency:.0f}ms"
                                    )
                            except Exception as e:
                                logger.warning(f"LLM è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é¢„è®¡ç®— fallback: {e}")
                                self._add_event("llm", f"LLM è°ƒç”¨å¼‚å¸¸ï¼Œé™çº§ä¸ºé¢„è®¡ç®—: {str(e)[:50]}")

                    # Fallback: ä½¿ç”¨é¢„è®¡ç®—çš„çŸ¥è¯†å›¾è°±
                    if anomaly_details is None and precomputed_kg:
                        anomaly_details = precomputed_kg
                        attribution_source = "precomputed"
                        self._add_event("llm", "ä½¿ç”¨é¢„è®¡ç®—çŸ¥è¯†å›¾è°± [Precomputed Fallback]")

                    # æ³¨å…¥å½’å› æ¥æºæ ‡è¯†
                    if anomaly_details:
                        anomaly_details = {
                            **anomaly_details,
                            "attribution_source": attribution_source
                        }

                point_data = AlignmentDataPoint(
                    timestamp=ts_str,
                    open=round(t["open"], 2),
                    high=round(t["high"], 2),
                    low=round(t["low"], 2),
                    close=round(t["close"], 2),
                    volume=round(t["volume"], 0),
                    hasAnomaly=is_anomaly,
                    anomalyDetails=anomaly_details,
                    detectionStats=detection if is_anomaly else None
                )

                await queue.put(point_data.model_dump())
                self.cache.increment_metric("total_ticks_pushed")

                # å›æ”¾æ§åˆ¶ï¼šæš‚åœç­‰å¾…
                while control_state.get("paused", False):
                    await asyncio.sleep(0.1)

                # å›æ”¾æ§åˆ¶ï¼šåŠ¨æ€é€Ÿåº¦
                speed = max(control_state.get("speed", 1.0), 0.1)
                await asyncio.sleep(1.0 / speed)

        producer_task = asyncio.create_task(tick_producer())

        try:
            while True:
                data = await queue.get()
                yield data
        finally:
            producer_task.cancel()
            db.close()
            self.cache.increment_metric("ws_connections", -1)
