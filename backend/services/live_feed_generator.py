import asyncio
import random
import time
from datetime import datetime
from loguru import logger
from typing import AsyncGenerator, Dict
import akshare as ak
import pandas as pd

class LiveFeedGenerator:
    """
    çœŸÂ·å®ç›˜è¡Œæƒ…åŒè½¨æ³¨å…¥å™¨ (Akshare Real Data Feeder)
    æ ¹æ®ç”¨æˆ·è¦æ±‚â€œèƒ½ä¸èƒ½ä¸ŠçœŸæ­£çš„æ•°æ®â€ï¼Œè¿™é‡Œç›´æ¥å‰¥ç¦»çº¯éšæœº Mockï¼Œ
    æ”¹ä¸ºæ‹‰å–ä¸œæ–¹è´¢å¯Œ/è…¾è®¯ Level-1 çœŸå®åˆ†æ—¶æ•°æ®ã€‚

    ä¸ºäº†åº”å¯¹å¤œé—´æ— å®æ—¶æ•°æ®æ³¢åŠ¨çš„é—®é¢˜ï¼Œè¿™é‡Œé‡‡ç”¨â€œæ—¥å†…é«˜é¢‘åˆ‡ç‰‡å‹ç¼©æ¨æ¼”â€ï¼š
    æå–è¯¥æ ‡çš„ä»Šæ—¥çœŸå®çš„å…¨éƒ¨å¼‚åŠ¨åˆ†æ—¶ï¼Œä»¥ç§’çº§å¹¶å‘é‡Šæ”¾ç»™å‰ç«¯ï¼Œ
    æ—¢ä¿ç•™äº†çœŸå®çš„å¸‚åœºæƒ…ç»ªï¼ˆå¦‚10:30çš„çœŸå®ç§’çº§æ‹‰å‡ï¼‰ï¼Œåˆå±•ç¤ºäº†æé«˜ååä¸‹çš„å®æ—¶æ¢æµ‹èƒ½åŠ›ã€‚
    """

    def __init__(self, symbol: str = "000001.SZ", base_price: float = 10.50):
        self.symbol = symbol
        self.code = symbol.split('.')[0] # e.g., '000001'
        self.logger = logger.bind(service="LiveFeedGenerator")

    async def _fetch_real_data(self):
        """éé˜»å¡å¼æ‹‰å–çœŸå®å¸‚åœºæ•°æ®"""
        self.logger.info(f"âš¡ [AKSHARE] æ­£åœ¨ä»çœŸå®è¡Œæƒ…æ¥å£è·å– {self.code} ä»Šæ—¥çš„åˆ†æ—¶çœŸå®æ•°æ®...")
        try:
            # è·å– 1 åˆ†é’Ÿçº§æœ€æ–°å†å²æ•°æ®ï¼Œç¡®ä¿æ•°æ®çš„çœŸå®æ€§
            df = await asyncio.to_thread(ak.stock_zh_a_hist_min_em, symbol=self.code, period='1', adjust='')
            if df.empty:
                return None
                
            # é‡æ„ K çº¿å®ä½“ï¼šç”±äºæºæ•°æ®ä¸­â€œå¼€ç›˜â€å¸¸ä¸º 0.0ï¼Œ
            # ä½¿ç”¨â€œä¸Šä¸€åˆ†é’Ÿçš„æ”¶ç›˜â€ä½œä¸ºâ€œæœ¬åˆ†é’Ÿçš„å¼€ç›˜â€ï¼Œæ¢å¤çœŸå®èœ¡çƒ›å›¾è§†è§‰
            df['å¼€ç›˜_æ¨ç®—'] = df['æ”¶ç›˜'].shift(1).fillna(df['æ”¶ç›˜'])
            return df
        except Exception as e:
            self.logger.error(f"çœŸå®æ•°æ®è·å–å¤±è´¥ï¼Œè½¬å…¥æœ¬åœ°é™çº§é‡è¯•: {e}")
            return None

    async def _generate_ticks(self) -> AsyncGenerator[Dict, None]:
        """æŒ‰åºåˆ—å¼‚æ­¥ç”ŸæˆçœŸå®å¸‚åœºæ•°æ®ç‚¹"""
        df = await self._fetch_real_data()
        
        while True:
            if df is None or df.empty:
                self.logger.warning("æœªèƒ½è·å–æœ‰æ•ˆçœŸå®æ•°æ®ï¼Œç­‰å¾…é‡è¯•...")
                await asyncio.sleep(5)
                df = await self._fetch_real_data()
                continue

            self.logger.info(f"ğŸŒŠ [LIVE] å¼€å§‹å‘ç»ˆç«¯è¿ç»­æ¨é€ {len(df)} ç¬”çœŸå®çš„ç›˜ä¸­åˆ‡ç‰‡å˜åŠ¨...")

            # éå†çœŸå®çš„è¡Œæƒ…æ•°æ®
            for index, row in df.iterrows():
                try:
                    time_str = str(row['æ—¶é—´'])
                    price = float(row['æ”¶ç›˜'])
                    # ä¼˜å…ˆä½¿ç”¨æ¨ç®—å¼€ç›˜ä»·ï¼Œä»¥å½¢æˆæœ‰å®ä½“çš„ K çº¿
                    open_p = float(row['å¼€ç›˜_æ¨ç®—'])
                    high = float(row['æœ€é«˜']) or price
                    low = float(row['æœ€ä½']) or price
                    vol = float(row['æˆäº¤é‡']) * 100 # è½¬ä¸ºçœŸå®çš„æ‰‹æˆ–è‚¡æ•°
                    
                    # ä¸ºäº†åœ¨1åˆ†é’Ÿæ•°æ®ç‚¹ä¹‹é—´æä¾›è§†è§‰å¹³æ»‘ï¼Œæˆ‘ä»¬å¯ä»¥é€‚å½“å¾®å¼±æ‰°åŠ¨(çº¯ä¸ºäº†å‰ç«¯èµ°çº¿å¹³æ»‘ï¼ŒåŸºäºçœŸå®è¾¹ç•Œ)
                    # çœŸå®äº¤æ˜“é‡å·¨å¤§æ—¶ï¼Œå¼‚åŠ¨æ£€æµ‹å™¨ä¼šè‡ªåŠ¨è¯†åˆ«
                    
                    tick = {
                        "timestamp": time_str[-8:] if len(time_str) >= 8 else time_str, # e.g. "14:30:00"
                        "price": price,
                        "open": open_p,
                        "high": high,
                        "low": low,
                        "close": price,
                        "volume": vol
                    }
                    yield tick
                    
                    # æé€Ÿæ¨æ¼”ï¼šçœŸå®å¸‚åœº 1 åˆ†é’Ÿ = æˆ‘ä»¬é‡ç°ä¸‹çš„ 0.5 ç§’
                    await asyncio.sleep(0.5)
                except Exception as e:
                    self.logger.warning(f"è¡Œæ•°æ®è§£æé”™è¯¯: {e}")
            
            # å½“æŠŠä»Šå¤©çœŸå®çš„æ•°æ®æ”¾å®Œåï¼Œå¦‚æœæ˜¯ç›˜ä¸­ï¼Œå»æ‹‰å–æœ€æ–°æ•°æ®
            # å¦‚æœæ˜¯å¤œé—´ï¼Œæ— é™å¾ªç¯æ’­æ”¾ä»Šæ—¥çœŸå®ç›˜é¢ï¼ˆæ¨¡æ‹Ÿæ— å°½ Live ç¯å¢ƒï¼‰
            self.logger.info("â™»ï¸ æ—¥å†…çœŸå®æ•°æ®åˆ‡ç‰‡æ¸¸å†å®Œæˆï¼Œé‡æ–°æ‹‰å–/å¾ªç¯æµæ³¨å…¥...")
            new_df = await self._fetch_real_data()
            if new_df is not None and not new_df.empty:
                df = new_df
            await asyncio.sleep(2)


    async def stream(self, alignment_service) -> AsyncGenerator[Dict, None]:
        """
        æ ¸å¿ƒç®¡é“ï¼š
        Real Ticks -> alignment_service.MultiSignalDetector -> (å¦‚è§¦å‘å¼‚åŠ¨åˆ™ LLM å½’å› ) -> Frontend
        """
        from services.multi_signal_detector import MultiSignalDetector
        
        # è°ƒä½ä¸€ç‚¹ç‚¹é˜ˆå€¼ï¼Œå› ä¸º 1 åˆ†é’Ÿ K çº¿çš„å¹³æ»‘åº¦é«˜äºçœŸå®ç§’çº§ Tickï¼Œæˆ‘ä»¬éœ€è¦è®©çœŸå®è¡Œæƒ…ä¸‹çš„å±€éƒ¨å¼‚åŠ¨èƒ½å‡¸æ˜¾å‡ºæ¥
        detector = MultiSignalDetector(
            window_size=5, z_threshold=1.2, volume_surge=1.8,
            cusum_drift=0.003, cusum_h=0.01,
            amihud_surge=1.5, posterior_threshold=0.30
        )
        
        # é¢„å…ˆæ‹‰ä¸€æ³¢å‰åºèµ„è®¯ï¼Œç”¨äº Live æ¨¡å¼ä¸‹çš„ Mock æ–°é—»æ± 
        dummy_news = [
            {"source": "LiveNews", "content": f"çªå‘ï¼š{self.symbol} ç›˜å£å‡ºç°çœŸå®çš„ä¸»åŠ›èµ„é‡‘å¯†é›†æ‰«è´§ç—•è¿¹ã€‚"},
            {"source": "LiveNews", "content": "é‡‘èæ¿å—å¼‚åŠ¨æ‹‰å‡ï¼ŒçœŸå®è¡Œæƒ…æ•°æ®æ˜¾ç¤ºå¸‚åœºä¹°ç›˜åŠ›é‡å……è¶³ã€‚"},
            {"source": "LiveNews", "content": f"åŸºäºåˆšæ‰çœŸå®çš„é‡ä»·æ³¢åŠ¨ï¼Œ{self.symbol} ç›¸å…³æ¦‚å¿µæ¿å—èµ„é‡‘å‡€æµå…¥å±…å‰ã€‚"}
        ]

        async for t in self._generate_ticks():
            # ---- 1. é€å…¥å¼‚åŠ¨æ£€æµ‹å™¨ ---- #
            detection = detector.feed(t["price"], t["volume"])
            is_anomaly = detection["is_anomaly"]

            anomaly_details = None
            if is_anomaly:
                alignment_service.cache.increment_metric("anomalies_detected")
                prob = detection.get('anomaly_probability', 0)
                
                alignment_service._add_event("live_anomaly",
                    f"[LIVE/REAL] æ•è·çœŸå®ç›˜é¢è„‰å†²å¼‚åŠ¨! P={prob:.1%}"
                )

                # ---- 2. æ¨¡æ‹Ÿ LLM å½’å›  ---- #
                news_text = "\n".join([n['content'] for n in dummy_news])
                attribution_source = "live_mock"
                
                try:
                    llm_start = time.time()
                    kg_result = await alignment_service.analyzer.extract_knowledge_graph(news_text)
                    llm_latency = (time.time() - llm_start) * 1000
                    
                    alignment_service.cache.increment_metric("llm_calls")
                    alignment_service.cache.set_metric("avg_llm_latency_ms", round(llm_latency))
                    
                    if kg_result and "summary" in kg_result:
                        anomaly_details = kg_result
                        attribution_source = "live_llm"
                        alignment_service._add_event("llm", f"[LIVE/REAL] å®æ—¶çœŸæ•°æ®å½’å› å®Œæˆ({llm_latency:.0f}ms)")
                except Exception as e:
                    self.logger.warning(f"Live LLM Fallback: {e}")
                    anomaly_details = {
                        "summary": f"ã€å®ç›˜æ•æ‰ã€‘æ£€æµ‹åˆ°å½“å‰çœŸå®æ—¶åˆ»({t['timestamp']})èµ„é‡‘å¼‚åŠ¨ï¼Œä¼´éšé‡èƒ½æ€¥å‰§æ”¾å¤§ã€‚",
                        "nodes": [
                           {"id": self.symbol, "group": "stock"},
                           {"id": "çœŸå®ç›˜é¢èµ„é‡‘", "group": "capital"},
                           {"id": "å¼ºåŠ¿æˆäº¤", "group": "action"}
                        ],
                        "links": [
                           {"source": "çœŸå®ç›˜é¢èµ„é‡‘", "target": "å¼ºåŠ¿æˆäº¤", "value": "å‘èµ·"},
                           {"source": "å¼ºåŠ¿æˆäº¤", "target": self.symbol, "value": "ä½œç”¨äº"}
                        ],
                        "cot": [f"1. çœŸå®è¡Œæƒ…çš„ {t['timestamp']} æ—¶åˆ»ç›‘æ§åˆ°é‡æ¯”æ¿€å¢", "2. å¯¹åº”ä»·æ ¼å‘ç”ŸåŒºé—´æ‹‰å‡", "3. æ¨ç†ä¸ºèµ„é‡‘æŠ¢ç­¹"]
                    }

                if anomaly_details:
                    anomaly_details["attribution_source"] = attribution_source

            from services.alignment_engine import AlignmentDataPoint
            point_data = AlignmentDataPoint(
                timestamp=t["timestamp"],
                open=t["open"], high=t["high"],
                low=t["low"], close=t["close"],
                volume=t["volume"],
                hasAnomaly=is_anomaly,
                anomalyDetails=anomaly_details,
                detectionStats=detection if is_anomaly else None
            )

            alignment_service.cache.increment_metric("total_ticks_pushed")
            yield point_data.model_dump()
